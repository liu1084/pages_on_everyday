## 激活函数



以下是一些常用的激活函数：

1. Sigmoid 函数:

$$

\sigma(x)=\frac{1}{1+e^{-x}}

$$



它将输入映射到范围 $(0,1)$ ，常用于二分类问题。

2. Hyperbolic Tangent 函数（tanh）:

$$

\tanh (x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}

$$



类似于 Sigmoid，但将输入映射到范围 $(-1,1)$ 。

3. ReLU 函数 (Rectified Linear Unit) :

$\operatorname{ReLU}(x)=\max (0, x)$

对于正数，输出等于输入；对于负数，输出为零。ReLU 是目前最常用的激活函数之

一，因为它在训练中通常具有更快的收敛速度。

4. Leaky ReLU 函数:

Leaky $\operatorname{ReLU}(x)=\max (\alpha x, x)$

其中 $\alpha$ 是一个小的正数，用于解决 ReLU 在负数部分的输出为零的问题。

5. Softmax 函数:

用于多分类问题，将一组数值映射到表示概率分布的向量。

每个激活函数都有其自身的特点和适用场景，选择合适的激活函数取决于具体的任务和网络架构。